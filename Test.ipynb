{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports, Variablen & Definitionen von Funktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version:  2.8.0\n",
      "Numpy version:  1.22.2\n",
      "Foolbox version:  3.3.1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#Imports\n",
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "import foolbox as fb\n",
    "from keras import callbacks\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from itertools import product\n",
    "from scipy.ndimage.interpolation import rotate, shift\n",
    "import csv\n",
    "\n",
    "\n",
    "#Variables\n",
    "epsilon=0.3\n",
    "batch_size=1024\n",
    "epochs=1000\n",
    "pgd_steps=50\n",
    "batch_count=0\n",
    "batch_count_inv=0\n",
    "print(\"Tensorflow version: \", tf.__version__)\n",
    "print(\"Numpy version: \", np.__version__)\n",
    "print(\"Foolbox version: \", fb.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "np.random.seed(10)\n",
    "\n",
    "\n",
    "#get MNIST data and prepare\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "img_rows = img_cols = 28\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "#define variables needed for attacks\n",
    "x_attack_to_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
    "x_attack_to_train=x_attack_to_train[:,:,:,np.newaxis]\n",
    "y_attack_to_train=tf.convert_to_tensor(y_train, dtype=tf.int32)\n",
    "\n",
    "x_attack_to_test = tf.convert_to_tensor(x_test, dtype=tf.float32)\n",
    "x_attack_to_test=x_attack_to_test[:,:,:,np.newaxis]\n",
    "y_attack_to_test=tf.convert_to_tensor(y_test, dtype=tf.int32)\n",
    "\n",
    "attack = fb.attacks.projected_gradient_descent.LinfProjectedGradientDescentAttack(steps=pgd_steps)\n",
    "\n",
    "#for generating invariance-based adversarial examples\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "#Functions\n",
    "def test_model(model):\n",
    "    \n",
    "    assert epsilon==0.3\n",
    "    inv_advs_to_test=np.load(\"data/invariance_examples_tramer/linf/automated_eps03.npy\")[0:100]\n",
    "    inv_labels_to_test=np.load(\"data/invariance_examples_tramer/linf/automated_eps03_labels.npy\")[0:100]\n",
    "    fmodel=fb.models.tensorflow.TensorFlowModel(model, bounds=(0,1))      \n",
    "    \n",
    "    x_batch,y_batch=x_test[0:100],y_test[0:100]\n",
    "    x_batch_to_test = tf.convert_to_tensor(x_batch, dtype=tf.float32)\n",
    "    y_batch_to_test=tf.convert_to_tensor(y_batch, dtype=tf.int32)\n",
    "\n",
    "    _,advs_to_test, success=attack(fmodel,x_batch_to_test, y_batch_to_test, epsilons=epsilon)\n",
    "   \n",
    "    success_rate=tf.keras.backend.get_value(success).mean(axis=-1).round(2)\n",
    "    x=tf.keras.backend.get_value(advs_to_test)\n",
    "    ptb_test=x\n",
    "\n",
    "    #get accuracies and losses\n",
    "    acc =model.evaluate(x_test[0:100],to_categorical(y_test[0:100]), verbose=0)\n",
    "    acc_ptb = model.evaluate(ptb_test,to_categorical(y_batch), verbose=0)\n",
    "    acc_inv = model.evaluate(inv_advs_to_test,to_categorical(inv_labels_to_test), verbose=0)\n",
    "\n",
    "\n",
    "    # get invariance adversarial examples success rate\n",
    "    predictions=model.predict(inv_advs_to_test)\n",
    "    disagreeing=0\n",
    "    for i in range(len(predictions)):\n",
    "        if inv_labels_to_test[i] !=np.argmax(predictions[i]):\n",
    "            disagreeing+=1\n",
    "      \n",
    "    return {\n",
    "    \"clean\":{\"loss\": acc[0], \"accuracy\":acc[1]},\n",
    "    \"ptb\":{\"loss\": acc_ptb[0], \"accuracy\":acc_ptb[1]},\n",
    "    \"inv\":{\"loss\": acc_inv[0], \"accuracy\":acc_inv[1]},\n",
    "    \"inv_success_rate\":disagreeing/100}\n",
    "\n",
    "\n",
    "def create_vanilla_model():\n",
    "      print(\"creating vanilla model...\")\n",
    "      \n",
    "      val_images = x_train[:10000]\n",
    "      partial_images = x_train[10000:]\n",
    "      val_labels = y_train[:10000]\n",
    "      partial_labels = y_train[10000:]\n",
    "\n",
    "      model = Sequential()\n",
    "\n",
    "      model.add(Conv2D(32, (5, 5), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n",
    "      model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "      model.add(Conv2D(64, (5, 5), activation='relu', kernel_initializer='he_uniform'))\n",
    "      model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "      model.add(Flatten())\n",
    "      model.add(Dense(1024, activation='relu', kernel_initializer='he_uniform'))\n",
    "      model.add(Dense(10, activation='softmax'))\n",
    "     \n",
    "      earlystopping = callbacks.EarlyStopping(monitor =\"val_loss\", \n",
    "                                        mode =\"min\", patience = 1, \n",
    "                                        restore_best_weights = True)\n",
    "\n",
    "      model.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "      print(\"training vanilla model...\")\n",
    "      history=model.fit(partial_images,to_categorical(partial_labels),\n",
    "                  validation_data =(val_images, to_categorical(val_labels)),\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  shuffle=True,\n",
    "                  verbose=2,\n",
    "                  callbacks =[earlystopping]\n",
    "                  )\n",
    "      print(np.shape(x_test))\n",
    "      acc = model.evaluate(x_test[0:100],to_categorical(y_test[0:100]))\n",
    "      print('BEFORE RETRAIN: Accuracy on clean testing data', acc[1])\n",
    "\n",
    "      return model\n",
    "\n",
    "def create_vanilla_model_tramer(filters=64, s1=5, s2=5, s3=3,\n",
    "               d1=0, d2=0, fc=256,\n",
    "               lr=1e-3, decay=1e-3):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters, kernel_size=(s1, s1),\n",
    "                     activation='relu',\n",
    "                     input_shape=(28, 28, 1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(filters*2, (s2, s2), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters*2, (s3, s3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(d1))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(fc, activation='relu'))\n",
    "    model.add(Dropout(d2))\n",
    "    model.add(Dense(10))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='Adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    final = Sequential()\n",
    "    final.add(model)\n",
    "    final.add(Activation('softmax'))\n",
    "    final.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='Adam',\n",
    "                  metrics=['accuracy'])\n",
    "        \n",
    "    final.fit(x_train, to_categorical(y_train, 10),\n",
    "              batch_size=256,\n",
    "              epochs=20,\n",
    "              shuffle=True,\n",
    "              verbose=2,\n",
    "    )\n",
    "    return final    \n",
    "\n",
    "def next_batch(data, labels, data_type):\n",
    "    if data_type==\"mnist\":\n",
    "        global batch_count\n",
    "        start=batch_count*100\n",
    "        end=(batch_count+1)*100\n",
    "        if batch_count<599:\n",
    "            batch_count+=1\n",
    "        else:\n",
    "            batch_count=0\n",
    "        \n",
    "        return data[start:end], labels[start:end]\n",
    "    if data_type==\"inv\":\n",
    "        global batch_count_inv\n",
    "        start=batch_count_inv*100\n",
    "        end=(batch_count_inv+1)*100\n",
    "        if batch_count_inv<4:\n",
    "            batch_count_inv+=1\n",
    "        else:\n",
    "            batch_count_inv=0\n",
    "        \n",
    "        return data[start:end], labels[start:end]\n",
    "\n",
    "\n",
    "# https://github.com/ftramer/Excessive-Invariance\n",
    "def linf_attack(x, nn_adv, eps):\n",
    "    x_adv = x.copy().astype(np.float32)\n",
    "    nn_adv = nn_adv.astype(np.float32)\n",
    "    \n",
    "    # if possible, change the pixels to the target value\n",
    "    idx = np.where((np.abs(nn_adv - x) <= eps*255.) & (x > 0))\n",
    "    x_adv[idx] = nn_adv[idx]\n",
    "    \n",
    "    # otherwise, go as close as possible\n",
    "    idx = np.where(np.abs(nn_adv - x) > eps*255.)\n",
    "    sign = np.sign(nn_adv - x)\n",
    "    x_adv[idx] += sign[idx] * eps * 255.\n",
    "    \n",
    "    x_adv = np.clip(x_adv, x.astype(np.float32) - eps*255, x.astype(np.float32) + eps*255)\n",
    "    x_adv = np.clip(x_adv, 0, 255.)\n",
    "    \n",
    "    return x_adv\n",
    "\n",
    "\n",
    "# https://github.com/ftramer/Excessive-Invariance\n",
    "# tries all rotation-translations of the input and returns the closest neighbor from each class\n",
    "def get_best_neighbors(x, y, all_NNs, grid):\n",
    "    xs = [shift(rotate(x, r, reshape=False), (tx, ty)).reshape(784) for (tx, ty, r) in grid]\n",
    "    xs = np.asarray(xs.copy())\n",
    "    \n",
    "    nns = []\n",
    "    y_nns = []\n",
    "    grids_nn = []\n",
    "    \n",
    "    # find a nearest neighbor in each class\n",
    "    for i in range(10):\n",
    "        if i != y:\n",
    "            X = X_train[Y_train == i]\n",
    "            Y = Y_train[Y_train == i]\n",
    "            distances, indices = all_NNs[i].kneighbors(xs, n_neighbors=1)\n",
    "\n",
    "            best = np.argmin(np.reshape(distances, -1))\n",
    "            best_idx = np.reshape(indices, -1)[best]\n",
    "            nns.append(X[best_idx])\n",
    "            y_nns.append(Y[best_idx])\n",
    "            \n",
    "            # store the inverse rotation+translation to be applied to the target\n",
    "            grids_nn.append(-np.asarray(grid[best]))\n",
    "    \n",
    "    return nns, y_nns, grids_nn\n",
    "\n",
    "\n",
    "# https://github.com/ftramer/Excessive-Invariance\n",
    "def generate_inv_adv_examples(epsilon_to_use, count, save):\n",
    "    import numpy as np\n",
    "    assert epsilon_to_use==0.3 or epsilon_to_use==0.4\n",
    "    \n",
    "    idxs=np.arange(0,count,1,dtype=int)\n",
    "\n",
    "    # Falsely the Invariance-Based Adversarial Examples are generated with the MNIST-Testing data but this doesn't matter because the Adversarial Examples are completely new generated images the Model has never seen.\n",
    "    # So it shouldn't matter if the Adversarial Examples are generated using the Testing-data or the Training-data.\n",
    "    # Saw this just at the end of the work. To fix this, I would have to ask the ten persons again to classify all 500 images and run all experiments again. \n",
    "    # Although the Models are tested only with 100 Examples \n",
    "    assert len(idxs) == count\n",
    "    test_xs = X_test[idxs]\n",
    "    test_ys = Y_test[idxs]\n",
    "\n",
    "    # build a nearest neighbors classifier per class\n",
    "    N = 1\n",
    "    all_NNs = []\n",
    "\n",
    "    for i in range(10):\n",
    "        #Reshape to 1D (28*28=784)\n",
    "        X = X_train[Y_train == i].reshape(-1, 784)\n",
    "        nn = NearestNeighbors(n_neighbors=N)\n",
    "    \n",
    "        nn.fit(X)\n",
    "        all_NNs.append(nn)\n",
    "    # print(all_NNs)\n",
    "\n",
    "\n",
    "\n",
    "    # Rotation-translation parameters\n",
    "    limits = [3, 3, 30]\n",
    "    granularity = [5, 5, 31]\n",
    "    grid = list(product(*list(np.linspace(-l, l, num=g) for l, g in zip(limits, granularity))))\n",
    "\n",
    "\n",
    "\n",
    "    all_nns = []\n",
    "    all_y_nns = []\n",
    "    all_grids_nns = []\n",
    "\n",
    "    # find nearest neighbors for some test inputs (this takes a little while)\n",
    "    for i in range(len(idxs)):\n",
    "        if i % 10 == 0:\n",
    "            print(\"{}/{} done\".format(i, len(idxs)))\n",
    "        x = test_xs[i]\n",
    "        y = test_ys[i]\n",
    "\n",
    "        # find the nearest neighbors for each class, with the corresponding rotation and translation\n",
    "        nns, y_nns, grids_nns = get_best_neighbors(x, y, all_NNs, grid)\n",
    "        nn_advs = [shift(rotate(nn, r, reshape=False), (tx, ty)) for (nn, (tx, ty, r)) in zip(nns, grids_nns)]\n",
    "        all_nns.append(nn_advs)\n",
    "        all_y_nns.append(y_nns)\n",
    "        all_grids_nns.append(np.asarray(grids_nns))\n",
    "\n",
    "    # save everything!\n",
    "    if(save==True):\n",
    "        np.save(\"data/invariance_examples_generation/X_test_{}.npy\".format(count), test_xs)\n",
    "        np.save(\"data/invariance_examples_generation/all_nns.npy\", np.asarray(all_nns))\n",
    "        np.save(\"data/invariance_examples_generation/all_y_nns.npy\", np.asarray(all_y_nns))\n",
    "        np.save(\"data/invariance_examples_generation/all_grids_nns.npy\", np.asarray(all_grids_nns))\n",
    "\n",
    "    all_nns=np.load(\"data/invariance_examples_generation/all_nns.npy\")\n",
    "    all_y_nns=np.load(\"data/invariance_examples_generation/all_y_nns.npy\")\n",
    "    all_grids_nns=np.load(\"data/invariance_examples_generation/all_grids_nns.npy\")\n",
    "    test_xs=np.load(\"data/invariance_examples_generation/X_test_{}.npy\".format(count))\n",
    "\n",
    "  \n",
    "    test_ys = y_test[idxs]\n",
    "\n",
    "    # manually chosen target classes for each source class\n",
    "    targets = {\n",
    "        0: [4, 6, 8, 9],\n",
    "        1: [4, 6, 7, 9],\n",
    "        2: [8],\n",
    "        3: [8],\n",
    "        4: [8, 9],\n",
    "        5: [3, 8],\n",
    "        6: [0],\n",
    "        7: [2, 3],\n",
    "        8: [3],\n",
    "        9: [3, 4, 5]\n",
    "    }\n",
    "\n",
    "    best_y_advs = []\n",
    "    best_targets = []\n",
    "    best_advs = []\n",
    "\n",
    "    for i in range(len(all_nns)):\n",
    "        x = test_xs[i]\n",
    "        y = test_ys[i]\n",
    "    \n",
    "        best_x_adv = None\n",
    "        best_nn_adv = None\n",
    "        amount_removed = []\n",
    "        amount_added = []\n",
    "        rot = []\n",
    "        best_y = None\n",
    "        min_removed = np.inf\n",
    "        for j in range(len(all_nns[i])):\n",
    "            nn_adv = all_nns[i][j]\n",
    "            y_nn = all_y_nns[i][j]\n",
    "            # print(\"NN ADV: {}\".format(np.shape(nn_adv)))\n",
    "            # print(\"X: {}\".format(np.shape(x)))\n",
    "            x_adv = linf_attack(x, nn_adv, epsilon_to_use)\n",
    "        \n",
    "            \n",
    "            # retain the target that required the least amount of pixels to be \"removed\"\n",
    "            curr_rot = np.abs(all_grids_nns[i][j][-1])\n",
    "            curr_removed = np.sum(np.abs(np.maximum(x/255. - x_adv/255., 0)))\n",
    "            \n",
    "            if y_nn in targets[y] and curr_removed < min_removed:\n",
    "                min_removed = curr_removed\n",
    "                best_y = y_nn\n",
    "                best_x_adv = x_adv\n",
    "                best_nn_adv = (nn_adv, y_nn)\n",
    "                    \n",
    "        best_targets.append(best_nn_adv)\n",
    "        best_advs.append(best_x_adv)\n",
    "        best_y_advs.append(best_y)\n",
    "        \n",
    "\n",
    "    if(save==True):\n",
    "        if epsilon_to_use==0.3:\n",
    "            np.save(\"data/invariance_examples/epsilon_0.3/invariance-based_adversarial_examples\", best_advs)\n",
    "            np.save(\"data/invariance_examples/epsilon_0.3/invariance-based_adversarial_examples_new_labels\", best_y_advs)\n",
    "        else:\n",
    "            np.save(\"data/invariance_examples/epsilon_0.4/invariance-based_adversarial_examples\", best_advs)\n",
    "            np.save(\"data/invariance_examples/epsilon_0.4/invariance-based_adversarial_examples_new_labels\", best_y_advs)\n",
    " \n",
    "\n",
    "        \n",
    "def ptb_training(ptb_acc_to_achieve, model_to_train, include_inv_training=False, inclusive_training=False, use_iterations=False, iterations=10):\n",
    "    if inclusive_training==True:\n",
    "        include_inv_training=False\n",
    "\n",
    "    inv_advs_to_train=np.load(\"data/invariance_examples/epsilon_0.3/invariance-based_adversarial_examples.npy\")\n",
    "    inv_labels_to_train=np.load(\"data/invariance_examples/epsilon_0.3/invariance-based_adversarial_examples_human_labels.npy\")\n",
    "    \n",
    "    earlystopping = callbacks.EarlyStopping(monitor =\"val_loss\", \n",
    "                                        mode =\"min\", patience = 1, \n",
    "                                       restore_best_weights = True)\n",
    "    #While ACCURACY\n",
    "    ptb_acc=0\n",
    "    i=0\n",
    "    y_axis=[]\n",
    "    x_axis_ptb=[]\n",
    "    x_axis_clean=[]\n",
    "    x_axis_inv=[]\n",
    "    if use_iterations==False:    \n",
    "        while ptb_acc<=ptb_acc_to_achieve:\n",
    "            res=test_model(model_to_train)\n",
    "            ptb_acc=res.get(\"ptb\").get(\"accuracy\")\n",
    "            clean_acc=res.get(\"clean\").get(\"accuracy\")\n",
    "            inv_acc=res.get(\"inv\").get(\"accuracy\")\n",
    "\n",
    "            i+=1\n",
    "            y_axis.append(i)\n",
    "            x_axis_ptb.append(ptb_acc)\n",
    "            x_axis_clean.append(clean_acc)\n",
    "            x_axis_inv.append(inv_acc)\n",
    "            fmodel=fb.models.tensorflow.TensorFlowModel(model_to_train, bounds=(0,1))   \n",
    "            x_batch,y_batch=next_batch(x_train,y_train, \"mnist\")\n",
    "            \n",
    "            x_batch_to_train = tf.convert_to_tensor(x_batch, dtype=tf.float32)\n",
    "            y_batch_to_train=tf.convert_to_tensor(y_batch, dtype=tf.int32)\n",
    "\n",
    "            #attack model    \n",
    "            _,advs, success=attack(fmodel, x_batch_to_train, y_batch_to_train, epsilons=epsilon) \n",
    "            success_rate=tf.keras.backend.get_value(success).mean(axis=-1).round(2)\n",
    "\n",
    "\n",
    "\n",
    "            if inclusive_training==True:\n",
    "                x=tf.keras.backend.get_value(advs)\n",
    "                x=x[:,:,:,0]\n",
    "\n",
    "                # perturbation based adversarial examples\n",
    "                x_training=x[0:int(len(x)*0.8)]\n",
    "                x_validation=x[int(len(x)*0.8):int(len(x))]\n",
    "                y_training=y_batch[0:int(len(x)*0.8)]\n",
    "                y_validation=y_batch[int(len(x)*0.8):int(len(x))]\n",
    "\n",
    "                # invariance based adversarial examples\n",
    "                x_inv,y_inv=next_batch(inv_advs_to_train,inv_labels_to_train, \"inv\")\n",
    "                x_inv_training=x_inv[0:int(len(x_inv)*0.8)]\n",
    "                x_inv_validation=x_inv[int(len(x_inv)*0.8):int(len(x_inv))]\n",
    "                y_inv_training=y_inv[0:int(len(y_inv)*0.8)]\n",
    "                y_inv_validation=y_inv[int(len(y_inv)*0.8):int(len(y_inv))]\n",
    "\n",
    "\n",
    "\n",
    "                # combine them into one array\n",
    "                x_training=np.append(x_training,x_inv_training, axis=0)\n",
    "                x_validation=np.append(x_validation,x_inv_validation, axis=0)\n",
    "                y_training=np.append(y_training,y_inv_training)\n",
    "                y_validation=np.append(y_validation,y_inv_validation)\n",
    "             \n",
    "               \n",
    "                \n",
    "                model_to_train.fit(x_training,to_categorical(y_training,num_classes=10),\n",
    "                    validation_data =(x_validation,to_categorical(y_validation, num_classes=10)),\n",
    "                    epochs=epochs,\n",
    "                    verbose=0,\n",
    "                    callbacks =[earlystopping])\n",
    "\n",
    "            else:         \n",
    "                #Retrain model with generated perturbation-based adversarial examples\n",
    "                #80% Training 20% Validation\n",
    "                x=tf.keras.backend.get_value(advs)\n",
    "                # print(\"Shape of x_training before reshape: {}\".format(np.shape(x)))\n",
    "                # print(\"Shape of x_training after reshape: {}\".format(np.shape(x)))\n",
    "                x_training=x[0:int(len(x)*0.8)]\n",
    "                x_validation=x[int(len(x)*0.8):int(len(x))]\n",
    "                y_training=y_batch[0:int(len(x)*0.8)]\n",
    "                y_validation=y_batch[int(len(x)*0.8):int(len(x))]\n",
    "                \n",
    "                model_to_train.fit(x_training,to_categorical(y_training,num_classes=10),\n",
    "                    validation_data =(x_validation,to_categorical(y_validation, num_classes=10)),\n",
    "                    epochs=epochs,\n",
    "                    verbose=0,\n",
    "                    callbacks =[earlystopping]\n",
    "                )\n",
    "\n",
    "                if include_inv_training==True:\n",
    "                    x_training=inv_advs_to_train[0:int(len(inv_advs_to_train)*0.8)]\n",
    "                    x_validation=inv_advs_to_train[int(len(inv_advs_to_train)*0.8):int(len(inv_advs_to_train))]\n",
    "                    y_training=inv_labels_to_train[0:int(len(inv_labels_to_train)*0.8)]\n",
    "                    y_validation=inv_labels_to_train[int(len(inv_labels_to_train)*0.8):int(len(inv_advs_to_train))]\n",
    "                    model_to_train.fit(x_training,to_categorical(y_training,num_classes=10),\n",
    "                        validation_data =(x_validation,to_categorical(y_validation, num_classes=10)),\n",
    "                        epochs=10,\n",
    "                        verbose=0,\n",
    "                        callbacks =[earlystopping]\n",
    "                    )\n",
    "            print(\"i: {} ptb acc: {}, inv_acc: {}\".format(i,ptb_acc, inv_acc))\n",
    "    else:\n",
    "        while i<iterations:\n",
    "            res=test_model(model_to_train)\n",
    "            ptb_acc=res.get(\"ptb\").get(\"accuracy\")\n",
    "            clean_acc=res.get(\"clean\").get(\"accuracy\")\n",
    "            inv_acc=res.get(\"inv\").get(\"accuracy\")\n",
    "\n",
    "            i+=1\n",
    "            y_axis.append(i)\n",
    "            x_axis_ptb.append(ptb_acc)\n",
    "            x_axis_clean.append(clean_acc)\n",
    "            x_axis_inv.append(inv_acc)\n",
    "            fmodel=fb.models.tensorflow.TensorFlowModel(model_to_train, bounds=(0,1))   \n",
    "            x_batch,y_batch=next_batch(x_train,y_train, \"mnist\")\n",
    "            \n",
    "            x_batch_to_train = tf.convert_to_tensor(x_batch, dtype=tf.float32)\n",
    "            y_batch_to_train=tf.convert_to_tensor(y_batch, dtype=tf.int32)\n",
    "\n",
    "            #attack model    \n",
    "            _,advs, success=attack(fmodel, x_batch_to_train, y_batch_to_train, epsilons=epsilon) \n",
    "            success_rate=tf.keras.backend.get_value(success).mean(axis=-1).round(2)\n",
    "            \n",
    "            if inclusive_training==True:\n",
    "                x=tf.keras.backend.get_value(advs)\n",
    "                x=x[:,:,:,0]\n",
    "\n",
    "                # perturbation based adversarial examples\n",
    "                x_training=x[0:int(len(x)*0.8)]\n",
    "                x_validation=x[int(len(x)*0.8):int(len(x))]\n",
    "                y_training=y_batch[0:int(len(x)*0.8)]\n",
    "                y_validation=y_batch[int(len(x)*0.8):int(len(x))]\n",
    "\n",
    "                # invariance based adversarial examples\n",
    "                x_inv,y_inv=next_batch(inv_advs_to_train,inv_labels_to_train, \"inv\")\n",
    "                x_inv_training=x_inv[0:int(len(x_inv)*0.8)]\n",
    "                x_inv_validation=x_inv[int(len(x_inv)*0.8):int(len(x_inv))]\n",
    "                y_inv_training=y_inv[0:int(len(y_inv)*0.8)]\n",
    "                y_inv_validation=y_inv[int(len(y_inv)*0.8):int(len(y_inv))]\n",
    "\n",
    "\n",
    "\n",
    "                # combine them into one array\n",
    "                x_training=np.append(x_training,x_inv_training, axis=0)\n",
    "                x_validation=np.append(x_validation,x_inv_validation, axis=0)\n",
    "                y_training=np.append(y_training,y_inv_training)\n",
    "                y_validation=np.append(y_validation,y_inv_validation)\n",
    "             \n",
    "               \n",
    "                \n",
    "                model_to_train.fit(x_training,to_categorical(y_training,num_classes=10),\n",
    "                    validation_data =(x_validation,to_categorical(y_validation, num_classes=10)),\n",
    "                    epochs=epochs,\n",
    "                    verbose=0,\n",
    "                    callbacks =[earlystopping]\n",
    "                )\n",
    "\n",
    "            else:         \n",
    "                #Retrain model with generated perturbation-based adversarial examples\n",
    "                #80% Training 20% Validation\n",
    "                x=tf.keras.backend.get_value(advs)\n",
    "                # print(\"Shape of x_training before reshape: {}\".format(np.shape(x)))\n",
    "                # print(\"Shape of x_training after reshape: {}\".format(np.shape(x)))\n",
    "                x_training=x[0:int(len(x)*0.8)]\n",
    "                x_validation=x[int(len(x)*0.8):int(len(x))]\n",
    "                y_training=y_batch[0:int(len(x)*0.8)]\n",
    "                y_validation=y_batch[int(len(x)*0.8):int(len(x))]\n",
    "                \n",
    "                model_to_train.fit(x_training,to_categorical(y_training,num_classes=10),\n",
    "                    validation_data =(x_validation,to_categorical(y_validation, num_classes=10)),\n",
    "                    epochs=epochs,\n",
    "                    verbose=0,\n",
    "                    callbacks =[earlystopping]\n",
    "                )\n",
    "                if include_inv_training==True:\n",
    "                    x_training=inv_advs_to_train[0:int(len(inv_advs_to_train)*0.8)]\n",
    "                    x_validation=inv_advs_to_train[int(len(inv_advs_to_train)*0.8):int(len(inv_advs_to_train))]\n",
    "                    y_training=inv_labels_to_train[0:int(len(inv_labels_to_train)*0.8)]\n",
    "                    y_validation=inv_labels_to_train[int(len(inv_labels_to_train)*0.8):int(len(inv_advs_to_train))]\n",
    "                    model_to_train.fit(x_training,to_categorical(y_training,num_classes=10),\n",
    "                        validation_data =(x_validation,to_categorical(y_validation, num_classes=10)),\n",
    "                        epochs=10,\n",
    "                        verbose=0,\n",
    "                        callbacks =[earlystopping]\n",
    "                    )\n",
    "        \n",
    "            print(\"i: {} ptb acc: {}, inv_acc: {}\".format(i,ptb_acc, inv_acc))\n",
    "    plt.plot( y_axis, x_axis_inv, label = \"INV\")\n",
    "    plt.plot( y_axis, x_axis_clean, label = \"Clean\")\n",
    "    plt.plot( y_axis, x_axis_ptb,label = \"PTB\")\n",
    "    plt.xlabel('Iterationen')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return {\n",
    "        \"model\": model_to_train,\n",
    "        \"clean\":{ \"accuracy\": x_axis_clean},\n",
    "        \"ptb\":{\"accuracy\":x_axis_ptb},\n",
    "        \"inv\":{\"accuracy\":x_axis_inv},\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erstelle/Trainiere das Vanilla Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_vanilla_model().save(\"models/vanilla_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greife Vanilla Modell an und Retrainiere mit Perturbation-Based Adversarial Examples iterativ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get Model\n",
    "model=load_model(\"models/vanilla_model\")\n",
    "\n",
    "ptb_acc_to_achieve=0.88\n",
    "model,ptb_acc=ptb_training(ptb_acc_to_achieve, model, use_iterations=True, iterations=100)\n",
    "model.save(\"models/ptb_trained_model_{}_ptb_accuracy\".format(ptb_acc)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gib PTB Adversarial Training Graph aus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_array(array):\n",
    "    filtered=[]\n",
    "    for i in range(len(array)):\n",
    "        if i%10==0:\n",
    "            filtered.append(array[i])\n",
    "    return filtered\n",
    "\n",
    "\n",
    "y=np.load(\"data/ptb_training/iteration_count_arr.npy\")\n",
    "clean_arr=np.load(\"data/ptb_training/clean_accuracy_arr.npy\")\n",
    "ptb_arr=np.load(\"data/ptb_training/ptb_accuracy_arr.npy\")\n",
    "plt.plot( y, clean_arr, label = \"Clean\")\n",
    "plt.plot( y, ptb_arr,label = \"PTB\")\n",
    "plt.xlabel('Iterationen')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "new_arr_x_ptb=filter_array(ptb_arr)\n",
    "new_arr_x_clean=filter_array(clean_arr)\n",
    "new_arr_y=filter_array(y)\n",
    "\n",
    "\n",
    "plt.plot( new_arr_y, new_arr_x_clean, label = \"Clean\")\n",
    "plt.plot( new_arr_y, new_arr_x_ptb,label = \"PTB\")\n",
    "plt.xlabel('Iterationen')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Max accuracy against PTB: {}\".format(np.max(ptb_arr)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generiere Invariance-Based Adversarial Examples\n",
    "Code ist von https://github.com/ftramer/Excessive-Invariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T UNCOMMENT THIS. this would overwrite the Invariance-Based Adversarial Examples!\n",
    "# generate_inv_adv_examples(0.3,500,True)\n",
    "# generate_inv_adv_examples(0.4,500,True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gib Invariance-Based-Based Adversarial Examples aus \n",
    "Diese Beispiele wurden von den zehn Personen angeschaut und die Labels wurden bestimmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_advs_to_train=np.load(\"data/invariance_examples/epsilon_0.3/invariance-based_adversarial_examples.npy\")\n",
    "inv_labels_to_train=np.load(\"data/invariance_examples/epsilon_0.3/invariance-based_adversarial_examples_new_labels.npy\")\n",
    "x_train\n",
    "\n",
    "print(\"----------EPSILON=0.3----------\")\n",
    "fig, axes = plt.subplots(50,10, figsize=(1.5*10,2*50))\n",
    "for i in range(500):\n",
    "    ax = axes[i//10,i%10]\n",
    "    ax.imshow(inv_advs_to_train[i], cmap='gray')\n",
    "    ax.set_title('Count: {}'.format(i))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "inv_advs_to_train=np.load(\"data/invariance_examples/epsilon_0.4/invariance-based_adversarial_examples.npy\")\n",
    "inv_labels_to_train=np.load(\"data/invariance_examples/epsilon_0.4/invariance-based_adversarial_examples_new_labels.npy\")\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"----------EPSILON=0.4----------\")\n",
    "fig, axes = plt.subplots(50,10, figsize=(1.5*10,2*50))\n",
    "for i in range(500):\n",
    "    ax = axes[i//10,i%10]\n",
    "    ax.imshow(inv_advs_to_train[i], cmap='gray')\n",
    "    ax.set_title('Count: {}'.format(i))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erster Durchlauf (Anzahl an Invariance-Based Adversarial Examples beim Trainieren variiert. Immer die neuen Labels verwenden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon\n",
    "epsilon=0.3\n",
    "\n",
    "c=[]\n",
    "i=500\n",
    "j=5\n",
    "while j<=i:\n",
    "    c.append(j)\n",
    "    j+=5\n",
    "\n",
    "vanilla_model=load_model(\"models/vanilla_model\")\n",
    "\n",
    "# m=l_infinity_PGD\n",
    "# a=88.9\n",
    "ptb_trained_model=load_model(\"models/ptb_trained_model_0.889_ptb_accuracy_PGD\")\n",
    "\n",
    "# Invariance-Based Adversarial Examples to train, use ONLY THE NEW LABELS\n",
    "inv_advs_to_train=np.load(\"data/invariance_examples/epsilon_0.3/invariance-based_adversarial_examples.npy\")\n",
    "inv_labels_to_train=np.load(\"data/invariance_examples/epsilon_0.3/invariance-based_adversarial_examples_new_labels.npy\")\n",
    "\n",
    "\n",
    "\n",
    "# Initialize writing results to csv\n",
    "handler_inv_trained = open('data/results/erster_durchlauf/inv_trained.csv', 'w',encoding='UTF8',newline='')\n",
    "writer_inv_trained = csv.writer(handler_inv_trained)\n",
    "writer_inv_trained.writerow([\"c\",\"clean_acc\",\"ptb_acc\", \"inv_acc\", \"clean_loss\", \"ptb_loss\", \"inv_loss\", \"inv_success_rate\" ])\n",
    "\n",
    "\n",
    "handler_ptb_inv_trained = open('data/results/erster_durchlauf/ptb_inv_trained.csv', 'w',encoding='UTF8',newline='')\n",
    "writer_ptb_inv_trained = csv.writer(handler_ptb_inv_trained)\n",
    "writer_ptb_inv_trained.writerow([\"c\",\"clean_acc\",\"ptb_acc\", \"inv_acc\", \"clean_loss\", \"ptb_loss\", \"inv_loss\", \"inv_success_rate\" ])\n",
    "\n",
    "\n",
    "\n",
    "initial_results_vanilla=test_model(vanilla_model)\n",
    "initial_results_ptb=test_model(ptb_trained_model)\n",
    "\n",
    "data=[0,initial_results_vanilla.get(\"clean\").get(\"accuracy\"),\n",
    "    initial_results_vanilla.get(\"ptb\").get(\"accuracy\"),\n",
    "    initial_results_vanilla.get(\"inv\").get(\"accuracy\"),\n",
    "    initial_results_vanilla.get(\"clean\").get(\"loss\"),\n",
    "    initial_results_vanilla.get(\"ptb\").get(\"loss\"),\n",
    "    initial_results_vanilla.get(\"inv\").get(\"loss\"),\n",
    "    initial_results_vanilla.get(\"inv_success_rate\"),\n",
    "    ]\n",
    "\n",
    "writer_inv_trained.writerow(data)\n",
    "\n",
    "data=[0,initial_results_ptb.get(\"clean\").get(\"accuracy\"),\n",
    "    initial_results_ptb.get(\"ptb\").get(\"accuracy\"),\n",
    "    initial_results_ptb.get(\"inv\").get(\"accuracy\"),\n",
    "    initial_results_ptb.get(\"clean\").get(\"loss\"),\n",
    "    initial_results_ptb.get(\"ptb\").get(\"loss\"),\n",
    "    initial_results_ptb.get(\"inv\").get(\"loss\"),\n",
    "    initial_results_ptb.get(\"inv_success_rate\"),\n",
    "    ]\n",
    "\n",
    "writer_ptb_inv_trained.writerow(data)\n",
    "\n",
    "\n",
    "print(\"Initial results from Vanilla Model: {}\".format(initial_results_vanilla))\n",
    "print(\"Initial results from PTB-Trained Model: {}\".format(initial_results_ptb))\n",
    "\n",
    "\n",
    "results_inv_trained=[]\n",
    "results_ptb_inv_trained=[]\n",
    "for i in range(len(c)):\n",
    "    print(\"Training with {} examples...\".format(c[i]))\n",
    "\n",
    "    vanilla_model.fit(inv_advs_to_train[0:c[i]],to_categorical(inv_labels_to_train[0:c[i]],num_classes=10),\n",
    "    epochs=10,\n",
    "    verbose=0)\n",
    "    \n",
    "\n",
    "    res=test_model(vanilla_model)\n",
    "    results_inv_trained.append(res)\n",
    "    data=[c[i],res.get(\"clean\").get(\"accuracy\"),\n",
    "    res.get(\"ptb\").get(\"accuracy\"),\n",
    "    res.get(\"inv\").get(\"accuracy\"),\n",
    "    res.get(\"clean\").get(\"loss\"),\n",
    "    res.get(\"ptb\").get(\"loss\"),\n",
    "    res.get(\"inv\").get(\"loss\"),\n",
    "    res.get(\"inv_success_rate\"),\n",
    "    ]\n",
    "    # write to csv file\n",
    "    writer_inv_trained.writerow(data)\n",
    "\n",
    "    ptb_trained_model.fit(inv_advs_to_train[0:c[i]],to_categorical(inv_labels_to_train[0:c[i]],num_classes=10),\n",
    "    epochs=10,\n",
    "    verbose=0)\n",
    "\n",
    "    res=test_model(ptb_trained_model)\n",
    "    results_ptb_inv_trained.append(res)\n",
    "    data=[c[i],res.get(\"clean\").get(\"accuracy\"),\n",
    "    res.get(\"ptb\").get(\"accuracy\"),\n",
    "    res.get(\"inv\").get(\"accuracy\"),\n",
    "    res.get(\"clean\").get(\"loss\"),\n",
    "    res.get(\"ptb\").get(\"loss\"),\n",
    "    res.get(\"inv\").get(\"loss\"),\n",
    "    res.get(\"inv_success_rate\"),\n",
    "    ]\n",
    "\n",
    "    #write to csv file\n",
    "    writer_ptb_inv_trained.writerow(data)\n",
    "\n",
    "    #reload models...\n",
    "    vanilla_model=load_model(\"models/vanilla_model\")\n",
    "    ptb_trained_model=load_model(\"models/ptb_trained_model_0.889_ptb_accuracy_PGD\")\n",
    "handler_ptb_inv_trained.close()\n",
    "handler_inv_trained.close()\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"----------Results INV-Trained Model----------\")\n",
    "i=0\n",
    "for entry in results_inv_trained:\n",
    "    print(\"Clean accuracy INV_trained with {} examples: {}\".format(c[i],entry.get(\"clean\").get(\"accuracy\")))\n",
    "    i+=1\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"----------Results PTB-INV-Trained Model----------\")\n",
    "i=0\n",
    "for entry in results_ptb_inv_trained:\n",
    "    print(\"Clean accuracy PTB-INV_trained with {} examples: {}\".format(c[i],entry.get(\"clean\").get(\"accuracy\")))\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ersten Durchlauf evaluieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INV-Trained\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (101,) and (0,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-2f202ea1eb75>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"INV-Trained\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minv_trained_clean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Clean\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minv_trained_ptb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"PTB\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minv_trained_inv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"INV\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2986\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2987\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2988\u001b[1;33m     return gca().plot(\n\u001b[0m\u001b[0;32m   2989\u001b[0m         \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2990\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1603\u001b[0m         \"\"\"\n\u001b[0;32m   1604\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1605\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1606\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    313\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 315\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 501\u001b[1;33m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[0;32m    502\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[0;32m    503\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (101,) and (0,)"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAANT0lEQVR4nO3cYYjkd33H8ffHO1NpjKb0VpC706T00njYQtIlTRFqirZc8uDugUXuIFgleGAbKVWEFEuU+MiGWhCu1ZOKVdAYfSALntwDjQTEC7chNXgXItvTeheFrDHNk6Ax7bcPZtKdrneZf3Zndy/7fb/gYP7/+e3Mlx97752d2ZlUFZKk7e8VWz2AJGlzGHxJasLgS1ITBl+SmjD4ktSEwZekJqYGP8lnkzyZ5PuXuD5JPplkKcmjSW6c/ZiSpPUa8gj/c8CBF7n+VmDf+N9R4F/WP5YkadamBr+qHgR+/iJLDgGfr5FTwNVJXj+rASVJs7FzBrexGzg/cXxhfO6nqxcmOcrotwCuvPLKP7z++utncPeS1MfDDz/8s6qaW8vXziL4g1XVceA4wPz8fC0uLm7m3UvSy16S/1zr187ir3SeAPZOHO8Zn5MkXUZmEfwF4F3jv9a5GXimqn7t6RxJ0taa+pROki8BtwC7klwAPgK8EqCqPgWcAG4DloBngfds1LCSpLWbGvyqOjLl+gL+emYTSZI2hO+0laQmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqYlBwU9yIMnjSZaS3HWR69+Q5IEkjyR5NMltsx9VkrQeU4OfZAdwDLgV2A8cSbJ/1bK/B+6vqhuAw8A/z3pQSdL6DHmEfxOwVFXnquo54D7g0Ko1BbxmfPm1wE9mN6IkaRaGBH83cH7i+ML43KSPArcnuQCcAN5/sRtKcjTJYpLF5eXlNYwrSVqrWb1oewT4XFXtAW4DvpDk1267qo5X1XxVzc/Nzc3oriVJQwwJ/hPA3onjPeNzk+4A7geoqu8CrwJ2zWJASdJsDAn+aWBfkmuTXMHoRdmFVWt+DLwNIMmbGAXf52wk6TIyNfhV9TxwJ3ASeIzRX+OcSXJPkoPjZR8E3pvke8CXgHdXVW3U0JKkl27nkEVVdYLRi7GT5+6euHwWeMtsR5MkzZLvtJWkJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNTEo+EkOJHk8yVKSuy6x5p1JziY5k+SLsx1TkrReO6ctSLIDOAb8GXABOJ1koarOTqzZB/wd8JaqejrJ6zZqYEnS2gx5hH8TsFRV56rqOeA+4NCqNe8FjlXV0wBV9eRsx5QkrdeQ4O8Gzk8cXxifm3QdcF2S7yQ5leTAxW4oydEki0kWl5eX1zaxJGlNZvWi7U5gH3ALcAT4TJKrVy+qquNVNV9V83NzczO6a0nSEEOC/wSwd+J4z/jcpAvAQlX9qqp+CPyA0Q8ASdJlYkjwTwP7klyb5ArgMLCwas3XGD26J8kuRk/xnJvdmJKk9Zoa/Kp6HrgTOAk8BtxfVWeS3JPk4HjZSeCpJGeBB4APVdVTGzW0JOmlS1VtyR3Pz8/X4uLilty3JL1cJXm4qubX8rW+01aSmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmBgU/yYEkjydZSnLXi6x7R5JKMj+7ESVJszA1+El2AMeAW4H9wJEk+y+y7irgb4CHZj2kJGn9hjzCvwlYqqpzVfUccB9w6CLrPgZ8HPjFDOeTJM3IkODvBs5PHF8Yn/s/SW4E9lbV11/shpIcTbKYZHF5efklDytJWrt1v2ib5BXAJ4APTltbVcerar6q5ufm5tZ715Kkl2BI8J8A9k4c7xmfe8FVwJuBbyf5EXAzsOALt5J0eRkS/NPAviTXJrkCOAwsvHBlVT1TVbuq6pqqugY4BRysqsUNmViStCZTg19VzwN3AieBx4D7q+pMknuSHNzoASVJs7FzyKKqOgGcWHXu7kusvWX9Y0mSZs132kpSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmhgU/CQHkjyeZCnJXRe5/gNJziZ5NMk3k7xx9qNKktZjavCT7ACOAbcC+4EjSfavWvYIMF9VfwB8FfiHWQ8qSVqfIY/wbwKWqupcVT0H3AccmlxQVQ9U1bPjw1PAntmOKUlaryHB3w2cnzi+MD53KXcA37jYFUmOJllMsri8vDx8SknSus30RdsktwPzwL0Xu76qjlfVfFXNz83NzfKuJUlT7Byw5glg78TxnvG5/yfJ24EPA2+tql/OZjxJ0qwMeYR/GtiX5NokVwCHgYXJBUluAD4NHKyqJ2c/piRpvaYGv6qeB+4ETgKPAfdX1Zkk9yQ5OF52L/Bq4CtJ/j3JwiVuTpK0RYY8pUNVnQBOrDp398Tlt894LknSjPlOW0lqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpoYFPwkB5I8nmQpyV0Xuf43knx5fP1DSa6Z+aSSpHWZGvwkO4BjwK3AfuBIkv2rlt0BPF1Vvwv8E/DxWQ8qSVqfIY/wbwKWqupcVT0H3AccWrXmEPBv48tfBd6WJLMbU5K0XjsHrNkNnJ84vgD80aXWVNXzSZ4Bfhv42eSiJEeBo+PDXyb5/lqG3oZ2sWqvGnMvVrgXK9yLFb+31i8cEvyZqarjwHGAJItVNb+Z93+5ci9WuBcr3IsV7sWKJItr/dohT+k8AeydON4zPnfRNUl2Aq8FnlrrUJKk2RsS/NPAviTXJrkCOAwsrFqzAPzl+PJfAN+qqprdmJKk9Zr6lM74Ofk7gZPADuCzVXUmyT3AYlUtAP8KfCHJEvBzRj8Upjm+jrm3G/dihXuxwr1Y4V6sWPNexAfiktSD77SVpCYMviQ1seHB92MZVgzYiw8kOZvk0STfTPLGrZhzM0zbi4l170hSSbbtn+QN2Ysk7xx/b5xJ8sXNnnGzDPg/8oYkDyR5ZPz/5LatmHOjJflskicv9V6ljHxyvE+PJrlx0A1X1Yb9Y/Qi738AvwNcAXwP2L9qzV8BnxpfPgx8eSNn2qp/A/fiT4HfHF9+X+e9GK+7CngQOAXMb/XcW/h9sQ94BPit8fHrtnruLdyL48D7xpf3Az/a6rk3aC/+BLgR+P4lrr8N+AYQ4GbgoSG3u9GP8P1YhhVT96KqHqiqZ8eHpxi952E7GvJ9AfAxRp/L9IvNHG6TDdmL9wLHquppgKp6cpNn3CxD9qKA14wvvxb4ySbOt2mq6kFGf/F4KYeAz9fIKeDqJK+fdrsbHfyLfSzD7kutqarngRc+lmG7GbIXk+5g9BN8O5q6F+NfUfdW1dc3c7AtMOT74jrguiTfSXIqyYFNm25zDdmLjwK3J7kAnADevzmjXXZeak+ATf5oBQ2T5HZgHnjrVs+yFZK8AvgE8O4tHuVysZPR0zq3MPqt78Ekv19V/7WVQ22RI8Dnquofk/wxo/f/vLmq/merB3s52OhH+H4sw4ohe0GStwMfBg5W1S83abbNNm0vrgLeDHw7yY8YPUe5sE1fuB3yfXEBWKiqX1XVD4EfMPoBsN0M2Ys7gPsBquq7wKsYfbBaN4N6stpGB9+PZVgxdS+S3AB8mlHst+vztDBlL6rqmaraVVXXVNU1jF7POFhVa/7QqMvYkP8jX2P06J4kuxg9xXNuE2fcLEP24sfA2wCSvIlR8Jc3dcrLwwLwrvFf69wMPFNVP532RRv6lE5t3McyvOwM3It7gVcDXxm/bv3jqjq4ZUNvkIF70cLAvTgJ/HmSs8B/Ax+qqm33W/DAvfgg8Jkkf8voBdx3b8cHiEm+xOiH/K7x6xUfAV4JUFWfYvT6xW3AEvAs8J5Bt7sN90qSdBG+01aSmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElq4n8BzPZculjwdYoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "handler_inv_trained = open('data/results/erster_durchlauf/inv_trained.csv', 'r')\n",
    "reader_inv_trained = csv.DictReader(handler_inv_trained)\n",
    "\n",
    "handler_ptb_inv_trained = open('data/results/erster_durchlauf/ptb_inv_trained.csv', 'r')\n",
    "reader_ptb_inv_trained = csv.DictReader(handler_ptb_inv_trained)\n",
    "\n",
    "inv_trained_clean=[]\n",
    "inv_trained_ptb=[]\n",
    "inv_trained_inv=[]\n",
    "\n",
    "ptb_inv_trained_clean=[]\n",
    "ptb_inv_trained_ptb=[]\n",
    "ptb_inv_trained_inv=[]\n",
    "\n",
    "line_count = 0\n",
    "for row in reader_inv_trained:\n",
    "    \n",
    "    inv_trained_clean.append(float(row[\"clean_acc\"]))        \n",
    "    inv_trained_ptb.append(float(row[\"ptb_acc\"]))\n",
    "    inv_trained_inv.append(float(row[\"inv_acc\"]))\n",
    "    line_count+=1\n",
    "\n",
    "line_count = 0\n",
    "for row in reader_ptb_inv_trained:\n",
    "    \n",
    "    ptb_inv_trained_clean.append(float(row[\"clean_acc\"]))\n",
    "    ptb_inv_trained_ptb.append(float(row[\"ptb_acc\"]))\n",
    "    ptb_inv_trained_inv.append(float(row[\"inv_acc\"]))\n",
    "    line_count+=1\n",
    "\n",
    "y=[]\n",
    "i=500\n",
    "j=0\n",
    "while j<=500:\n",
    "    y.append(j)\n",
    "    j+=5\n",
    "\n",
    "print(\"INV-Trained\")  \n",
    "plt.plot( y, inv_trained_clean, label = \"Clean\")\n",
    "plt.plot( y, inv_trained_ptb,label = \"PTB\")\n",
    "plt.plot( y, inv_trained_inv,label = \"INV\")\n",
    "plt.xlabel('c')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"PTB-INV-Trained\")\n",
    "plt.plot( y, ptb_inv_trained_clean, label = \"Clean\")\n",
    "plt.plot( y, ptb_inv_trained_ptb,label = \"PTB\")\n",
    "plt.plot( y, ptb_inv_trained_inv,label = \"INV\")\n",
    "plt.xlabel('c')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "inv_trained_clean={\n",
    "    \"initial\": inv_trained_clean[0],\n",
    "    \"mean\":np.mean(inv_trained_clean),\n",
    "    \"min\":np.min(inv_trained_clean),\n",
    "    \"max\":np.max(inv_trained_clean)\n",
    "}\n",
    "\n",
    "inv_trained_ptb={\n",
    "    \"initial\": inv_trained_ptb[0],\n",
    "    \"mean\":np.mean(inv_trained_ptb),\n",
    "    \"min\":np.min(inv_trained_ptb),\n",
    "    \"max\":np.max(inv_trained_ptb)\n",
    "}\n",
    "\n",
    "inv_trained_inv={\n",
    "    \"initial\": inv_trained_inv[0],\n",
    "    \"mean\":np.mean(inv_trained_inv),\n",
    "    \"min\":np.min(inv_trained_inv),\n",
    "    \"max\":np.max(inv_trained_inv)\n",
    "}\n",
    "\n",
    "\n",
    "ptb_inv_trained_clean={\n",
    "    \"initial\": ptb_inv_trained_clean[0],\n",
    "    \"mean\":np.mean(ptb_inv_trained_clean),\n",
    "    \"min\":np.min(ptb_inv_trained_clean),\n",
    "    \"max\":np.max(ptb_inv_trained_clean)\n",
    "}\n",
    "\n",
    "ptb_inv_trained_ptb={\n",
    "    \"initial\": ptb_inv_trained_ptb[0],\n",
    "    \"mean\":np.mean(ptb_inv_trained_ptb),\n",
    "    \"min\":np.min(ptb_inv_trained_ptb),\n",
    "    \"max\":np.max(ptb_inv_trained_ptb)\n",
    "}\n",
    "\n",
    "ptb_inv_trained_inv={\n",
    "    \"initial\": ptb_inv_trained_inv[0],\n",
    "    \"mean\":np.mean(ptb_inv_trained_inv),\n",
    "    \"min\":np.min(ptb_inv_trained_inv),\n",
    "    \"max\":np.max(ptb_inv_trained_inv)\n",
    "}\n",
    "\n",
    "print(\"INV_TRAINED\")\n",
    "print()\n",
    "print(\"INV-TRAINED CLEAN: initial: {}, min: {}, max: {}, mean: {}\".format(inv_trained_clean.get(\"initial\"),inv_trained_clean.get(\"min\"),inv_trained_clean.get(\"max\"),inv_trained_clean.get(\"mean\")))\n",
    "print(\"INV-TRAINED PTB: initial: {}, min: {}, max: {}, mean: {}\".format(inv_trained_ptb.get(\"initial\"),inv_trained_ptb.get(\"min\"),inv_trained_ptb.get(\"max\"),inv_trained_ptb.get(\"mean\")))\n",
    "print(\"INV-TRAINED INV: initial: {}, min: {}, max: {}, mean: {}\".format(inv_trained_inv.get(\"initial\"),inv_trained_inv.get(\"min\"),inv_trained_inv.get(\"max\"),inv_trained_inv.get(\"mean\")))\n",
    "print()\n",
    "print()\n",
    "print(\"PTB-INV_TRAINED\")\n",
    "print()\n",
    "print(\"PTB_INV-TRAINED CLEAN: initial: {}, min: {}, max: {}, mean: {}\".format(ptb_inv_trained_clean.get(\"initial\"),ptb_inv_trained_clean.get(\"min\"),ptb_inv_trained_clean.get(\"max\"),ptb_inv_trained_clean.get(\"mean\")))\n",
    "print(\"PTB_INV-TRAINED PTB: initial: {}, min: {}, max: {}, mean: {}\".format(ptb_inv_trained_ptb.get(\"initial\"),ptb_inv_trained_ptb.get(\"min\"),ptb_inv_trained_ptb.get(\"max\"),ptb_inv_trained_ptb.get(\"mean\")))\n",
    "print(\"PTB_INV-TRAINED INV: initial: {}, min: {}, max: {}, mean: {}\".format(ptb_inv_trained_inv.get(\"initial\"),ptb_inv_trained_inv.get(\"min\"),ptb_inv_trained_inv.get(\"max\"),ptb_inv_trained_inv.get(\"mean\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zweiter Durchlauf (Dasselbe wie beim ersten Durchlauf mit dem Unterschied, dass die Labels beim Retrainieren mit Invariance-Based Adversarial Examples von zehn Personen bestimmt wurden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon\n",
    "epsilon=0.3\n",
    "\n",
    "# c\n",
    "c=[]\n",
    "i=500\n",
    "j=5\n",
    "while j<=i:\n",
    "    c.append(j)\n",
    "    j+=5\n",
    "\n",
    "\n",
    "vanilla_model=load_model(\"models/vanilla_model\")\n",
    "\n",
    "# m=l_infinity_PGD\n",
    "# a=88.9\n",
    "ptb_trained_model=load_model(\"models/ptb_trained_model_0.889_ptb_accuracy_PGD\")\n",
    "\n",
    "# Invariance-Based Adversarial Examples to train, use ONLY THE NEW LABELS\n",
    "inv_advs_to_train=np.load(\"data/invariance_examples/epsilon_0.3/invariance-based_adversarial_examples.npy\")\n",
    "inv_labels_to_train=np.load(\"data/invariance_examples/epsilon_0.3/invariance-based_adversarial_examples_human_labels.npy\")\n",
    "\n",
    "print(np.shape(inv_labels_to_train))\n",
    "\n",
    "# Initialize writing results to csv\n",
    "handler_inv_trained = open('data/results/zweiter_durchlauf/inv_trained.csv', 'w',encoding='UTF8',newline='')\n",
    "writer_inv_trained = csv.writer(handler_inv_trained)\n",
    "writer_inv_trained.writerow([\"c\",\"clean_acc\",\"ptb_acc\", \"inv_acc\", \"clean_loss\", \"ptb_loss\", \"inv_loss\", \"inv_success_rate\" ])\n",
    "\n",
    "\n",
    "handler_ptb_inv_trained = open('data/results/zweiter_durchlauf/ptb_inv_trained.csv', 'w',encoding='UTF8',newline='')\n",
    "writer_ptb_inv_trained = csv.writer(handler_ptb_inv_trained)\n",
    "writer_ptb_inv_trained.writerow([\"c\",\"clean_acc\",\"ptb_acc\", \"inv_acc\", \"clean_loss\", \"ptb_loss\", \"inv_loss\", \"inv_success_rate\" ])\n",
    "\n",
    "initial_results_vanilla=test_model(vanilla_model)\n",
    "initial_results_ptb=test_model(ptb_trained_model)\n",
    "\n",
    "data=[0,initial_results_vanilla.get(\"clean\").get(\"accuracy\"),\n",
    "    initial_results_vanilla.get(\"ptb\").get(\"accuracy\"),\n",
    "    initial_results_vanilla.get(\"inv\").get(\"accuracy\"),\n",
    "    initial_results_vanilla.get(\"clean\").get(\"loss\"),\n",
    "    initial_results_vanilla.get(\"ptb\").get(\"loss\"),\n",
    "    initial_results_vanilla.get(\"inv\").get(\"loss\"),\n",
    "    initial_results_vanilla.get(\"inv_success_rate\"),\n",
    "    ]\n",
    "\n",
    "writer_inv_trained.writerow(data)\n",
    "\n",
    "data=[0,initial_results_ptb.get(\"clean\").get(\"accuracy\"),\n",
    "    initial_results_ptb.get(\"ptb\").get(\"accuracy\"),\n",
    "    initial_results_ptb.get(\"inv\").get(\"accuracy\"),\n",
    "    initial_results_ptb.get(\"clean\").get(\"loss\"),\n",
    "    initial_results_ptb.get(\"ptb\").get(\"loss\"),\n",
    "    initial_results_ptb.get(\"inv\").get(\"loss\"),\n",
    "    initial_results_ptb.get(\"inv_success_rate\"),\n",
    "    ]\n",
    "\n",
    "writer_ptb_inv_trained.writerow(data)\n",
    "\n",
    "print(\"Initial results from Vanilla Model: {}\".format(initial_results_vanilla))\n",
    "print(\"Initial results from PTB-Trained Model: {}\".format(initial_results_ptb))\n",
    "\n",
    "\n",
    "results_inv_trained=[]\n",
    "results_ptb_inv_trained=[]\n",
    "for i in range(len(c)):\n",
    "    print(\"Training with {} examples...\".format(c[i]))\n",
    "\n",
    "    vanilla_model.fit(inv_advs_to_train[0:c[i]],to_categorical(inv_labels_to_train[0:c[i]],num_classes=10),\n",
    "    epochs=10,\n",
    "    verbose=0)\n",
    "    \n",
    "    res=test_model(vanilla_model)\n",
    "    results_inv_trained.append(res)\n",
    "    data=[c[i],res.get(\"clean\").get(\"accuracy\"),\n",
    "    res.get(\"ptb\").get(\"accuracy\"),\n",
    "    res.get(\"inv\").get(\"accuracy\"),\n",
    "    res.get(\"clean\").get(\"loss\"),\n",
    "    res.get(\"ptb\").get(\"loss\"),\n",
    "    res.get(\"inv\").get(\"loss\"),\n",
    "    res.get(\"inv_success_rate\"),\n",
    "    ]\n",
    "    # write to csv file\n",
    "    writer_inv_trained.writerow(data)\n",
    "\n",
    "    ptb_trained_model.fit(inv_advs_to_train[0:c[i]],to_categorical(inv_labels_to_train[0:c[i]],num_classes=10),\n",
    "    epochs=10,\n",
    "    verbose=0)\n",
    "\n",
    "    res=test_model(ptb_trained_model)\n",
    "    results_ptb_inv_trained.append(res)\n",
    "    data=[c[i],res.get(\"clean\").get(\"accuracy\"),\n",
    "    res.get(\"ptb\").get(\"accuracy\"),\n",
    "    res.get(\"inv\").get(\"accuracy\"),\n",
    "    res.get(\"clean\").get(\"loss\"),\n",
    "    res.get(\"ptb\").get(\"loss\"),\n",
    "    res.get(\"inv\").get(\"loss\"),\n",
    "    res.get(\"inv_success_rate\"),\n",
    "    ]\n",
    "\n",
    "    #write to csv file\n",
    "    writer_ptb_inv_trained.writerow(data)\n",
    "\n",
    "    #reload models...\n",
    "    vanilla_model=load_model(\"models/vanilla_model\")\n",
    "    ptb_trained_model=load_model(\"models/ptb_trained_model_0.889_ptb_accuracy_PGD\")\n",
    "\n",
    "handler_ptb_inv_trained.close()\n",
    "handler_inv_trained.close()\n",
    "\n",
    "print()\n",
    "print(\"----------Results INV-Trained Model----------\")\n",
    "i=0\n",
    "for entry in results_inv_trained:\n",
    "    print(\"Clean accuracy INV_trained with {} examples: {}\".format(c[i],entry.get(\"clean\").get(\"accuracy\")))\n",
    "    i+=1\n",
    "\n",
    "print()\n",
    "print(\"----------Results PTB-INV-Trained Model----------\")\n",
    "i=0\n",
    "for entry in results_ptb_inv_trained:\n",
    "    print(\"Clean accuracy PTB-INV_trained with {} examples: {}\".format(c[i],entry.get(\"clean\").get(\"accuracy\")))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zweiten Durchlauf evaluieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "handler_inv_trained = open('data/results/zweiter_durchlauf/inv_trained.csv', 'r')\n",
    "reader_inv_trained = csv.DictReader(handler_inv_trained)\n",
    "\n",
    "handler_ptb_inv_trained = open('data/results/zweiter_durchlauf/ptb_inv_trained.csv', 'r')\n",
    "reader_ptb_inv_trained = csv.DictReader(handler_ptb_inv_trained)\n",
    "\n",
    "inv_trained_clean=[]\n",
    "inv_trained_ptb=[]\n",
    "inv_trained_inv=[]\n",
    "\n",
    "ptb_inv_trained_clean=[]\n",
    "ptb_inv_trained_ptb=[]\n",
    "ptb_inv_trained_inv=[]\n",
    "\n",
    "\n",
    "line_count = 0\n",
    "for row in reader_inv_trained:\n",
    "    \n",
    "    inv_trained_clean.append(float(row[\"clean_acc\"]))        \n",
    "    inv_trained_ptb.append(float(row[\"ptb_acc\"]))\n",
    "    inv_trained_inv.append(float(row[\"inv_acc\"]))\n",
    "    line_count+=1\n",
    "\n",
    "line_count = 0\n",
    "for row in reader_ptb_inv_trained:\n",
    "    \n",
    "    ptb_inv_trained_clean.append(float(row[\"clean_acc\"]))\n",
    "    ptb_inv_trained_ptb.append(float(row[\"ptb_acc\"]))\n",
    "    ptb_inv_trained_inv.append(float(row[\"inv_acc\"]))\n",
    "    line_count+=1\n",
    "\n",
    "y=[]\n",
    "i=500\n",
    "j=0\n",
    "while j<=500:\n",
    "    y.append(j)\n",
    "    j+=5\n",
    "\n",
    "print(\"INV-Trained\")  \n",
    "plt.plot( y, inv_trained_clean, label = \"Clean\")\n",
    "plt.plot( y, inv_trained_ptb,label = \"PTB\")\n",
    "plt.plot( y, inv_trained_inv,label = \"INV\")\n",
    "plt.xlabel('c')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"PTB-INV-Trained\")\n",
    "plt.plot( y, ptb_inv_trained_clean, label = \"Clean\")\n",
    "plt.plot( y, ptb_inv_trained_ptb,label = \"PTB\")\n",
    "plt.plot( y, ptb_inv_trained_inv,label = \"INV\")\n",
    "plt.xlabel('c')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "inv_trained_clean={\n",
    "    \"initial\": inv_trained_clean[0],\n",
    "    \"mean\":np.mean(inv_trained_clean),\n",
    "    \"min\":np.min(inv_trained_clean),\n",
    "    \"max\":np.max(inv_trained_clean)\n",
    "}\n",
    "\n",
    "inv_trained_ptb={\n",
    "    \"initial\": inv_trained_ptb[0],\n",
    "    \"mean\":np.mean(inv_trained_ptb),\n",
    "    \"min\":np.min(inv_trained_ptb),\n",
    "    \"max\":np.max(inv_trained_ptb)\n",
    "}\n",
    "\n",
    "inv_trained_inv={\n",
    "    \"initial\": inv_trained_inv[0],\n",
    "    \"mean\":np.mean(inv_trained_inv),\n",
    "    \"min\":np.min(inv_trained_inv),\n",
    "    \"max\":np.max(inv_trained_inv)\n",
    "}\n",
    "\n",
    "\n",
    "ptb_inv_trained_clean={\n",
    "    \"initial\": ptb_inv_trained_clean[0],\n",
    "    \"mean\":np.mean(ptb_inv_trained_clean),\n",
    "    \"min\":np.min(ptb_inv_trained_clean),\n",
    "    \"max\":np.max(ptb_inv_trained_clean)\n",
    "}\n",
    "\n",
    "ptb_inv_trained_ptb={\n",
    "    \"initial\": ptb_inv_trained_ptb[0],\n",
    "    \"mean\":np.mean(ptb_inv_trained_ptb),\n",
    "    \"min\":np.min(ptb_inv_trained_ptb),\n",
    "    \"max\":np.max(ptb_inv_trained_ptb)\n",
    "}\n",
    "\n",
    "ptb_inv_trained_inv={\n",
    "    \"initial\": ptb_inv_trained_inv[0],\n",
    "    \"mean\":np.mean(ptb_inv_trained_inv),\n",
    "    \"min\":np.min(ptb_inv_trained_inv),\n",
    "    \"max\":np.max(ptb_inv_trained_inv)\n",
    "}\n",
    "\n",
    "print(\"INV_TRAINED\")\n",
    "print()\n",
    "print(\"INV-TRAINED CLEAN: initial: {}, min: {}, max: {}, mean: {}\".format(inv_trained_clean.get(\"initial\"),inv_trained_clean.get(\"min\"),inv_trained_clean.get(\"max\"),inv_trained_clean.get(\"mean\")))\n",
    "print(\"INV-TRAINED PTB: initial: {}, min: {}, max: {}, mean: {}\".format(inv_trained_ptb.get(\"initial\"),inv_trained_ptb.get(\"min\"),inv_trained_ptb.get(\"max\"),inv_trained_ptb.get(\"mean\")))\n",
    "print(\"INV-TRAINED INV: initial: {}, min: {}, max: {}, mean: {}\".format(inv_trained_inv.get(\"initial\"),inv_trained_inv.get(\"min\"),inv_trained_inv.get(\"max\"),inv_trained_inv.get(\"mean\")))\n",
    "print()\n",
    "print()\n",
    "print(\"PTB-INV_TRAINED\")\n",
    "print()\n",
    "print(\"PTB_INV-TRAINED CLEAN: initial: {}, min: {}, max: {}, mean: {}\".format(ptb_inv_trained_clean.get(\"initial\"),ptb_inv_trained_clean.get(\"min\"),ptb_inv_trained_clean.get(\"max\"),ptb_inv_trained_clean.get(\"mean\")))\n",
    "print(\"PTB_INV-TRAINED PTB: initial: {}, min: {}, max: {}, mean: {}\".format(ptb_inv_trained_ptb.get(\"initial\"),ptb_inv_trained_ptb.get(\"min\"),ptb_inv_trained_ptb.get(\"max\"),ptb_inv_trained_ptb.get(\"mean\")))\n",
    "print(\"PTB_INV-TRAINED INV: initial: {}, min: {}, max: {}, mean: {}\".format(ptb_inv_trained_inv.get(\"initial\"),ptb_inv_trained_inv.get(\"min\"),ptb_inv_trained_inv.get(\"max\"),ptb_inv_trained_inv.get(\"mean\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dritter Durchlauf (PTB-INV Trained oder INV-PTB Trained?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon\n",
    "epsilon=0.3\n",
    "iterations=1500\n",
    "ptb_acc_to_achieve=1\n",
    "\n",
    "# Invariance-Based Adversarial Examples to train, use ONLY THE NEW LABELS\n",
    "inv_advs_to_train=np.load(\"data/invariance_examples/epsilon_0.3/invariance-based_adversarial_examples.npy\")\n",
    "inv_labels_to_train=np.load(\"data/invariance_examples/epsilon_0.3/invariance-based_adversarial_examples_human_labels.npy\")\n",
    "\n",
    "\n",
    "\n",
    "#Handler and writer...\n",
    "handler_simultan_trained = open('data/results/dritter_durchlauf/simultan.csv', 'w',encoding='UTF8',newline='')\n",
    "writer_simultan_trained = csv.writer(handler_simultan_trained)\n",
    "writer_simultan_trained.writerow([\"i\",\"clean_acc\",\"ptb_acc\", \"inv_acc\"])\n",
    "\n",
    "handler_inclusive_trained = open('data/results/dritter_durchlauf/inclusive.csv', 'w',encoding='UTF8',newline='')\n",
    "writer_inclusive_trained = csv.writer(handler_inclusive_trained)\n",
    "writer_inclusive_trained.writerow([\"i\",\"clean_acc\",\"ptb_acc\", \"inv_acc\"])\n",
    "\n",
    "handler_inv_ptb_trained = open('data/results/dritter_durchlauf/inv_ptb.csv', 'w',encoding='UTF8',newline='')\n",
    "writer_inv_ptb_trained = csv.writer(handler_inv_ptb_trained)\n",
    "writer_inv_ptb_trained.writerow([\"i\",\"clean_acc\",\"ptb_acc\", \"inv_acc\", ])\n",
    "\n",
    "handler_ptb_inv_trained = open('data/results/dritter_durchlauf/ptb_inv.csv', 'w',encoding='UTF8',newline='')\n",
    "writer_ptb_inv_trained = csv.writer(handler_ptb_inv_trained)\n",
    "writer_ptb_inv_trained.writerow([\"i\",\"clean_acc\",\"ptb_acc\", \"inv_acc\", ])\n",
    "\n",
    "# Inclusive training\n",
    "vanilla_model=load_model(\"models/vanilla_model\")\n",
    "res=ptb_training(ptb_acc_to_achieve, vanilla_model, include_inv_training=False, inclusive_training=True, use_iterations=True, iterations=iterations)\n",
    "\n",
    "ptb_acc_arr_inclusive=res.get(\"ptb\").get(\"accuracy\")\n",
    "inv_acc_arr_inclusive=res.get(\"inv\").get(\"accuracy\")\n",
    "clean_acc_arr_inclusive=res.get(\"clean\").get(\"accuracy\")\n",
    "\n",
    "for i in range(iterations):\n",
    "     data=[i,clean_acc_arr_inclusive[i],ptb_acc_arr_inclusive[i],inv_acc_arr_inclusive[i]]\n",
    "     writer_inclusive_trained.writerow(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Simultan training\n",
    "vanilla_model=load_model(\"models/vanilla_model\")\n",
    "res=ptb_training(ptb_acc_to_achieve, vanilla_model, include_inv_training=True, use_iterations=True, iterations=iterations)\n",
    "\n",
    "ptb_acc_arr_simultan=res.get(\"ptb\").get(\"accuracy\")\n",
    "inv_acc_arr_simultan=res.get(\"inv\").get(\"accuracy\")\n",
    "clean_acc_arr_simultan=res.get(\"clean\").get(\"accuracy\")\n",
    "\n",
    "\n",
    "for i in range(iterations):\n",
    "    data=[i,clean_acc_arr_simultan[i],ptb_acc_arr_simultan[i],inv_acc_arr_simultan[i]]\n",
    "    writer_simultan_trained.writerow(data)\n",
    "\n",
    "vanilla_model=load_model(\"models/vanilla_model\")\n",
    "\n",
    "\n",
    "\n",
    "# first INV-Training...\n",
    "print(\"INV-Training\")\n",
    "vanilla_model.fit(inv_advs_to_train,to_categorical(inv_labels_to_train,num_classes=10),\n",
    "epochs=10,\n",
    "verbose=0)\n",
    "\n",
    "result=test_model(vanilla_model)\n",
    "\n",
    "data=[iterations+1,result.get(\"clean\").get(\"accuracy\"),result.get(\"ptb\").get(\"accuracy\"),result.get(\"inv\").get(\"accuracy\")]\n",
    "writer_inv_ptb_trained.writerow(data)\n",
    "\n",
    "\n",
    "\n",
    "# # then PTB-Training\n",
    "res=ptb_training(ptb_acc_to_achieve, vanilla_model,use_iterations=True, iterations=iterations)\n",
    "\n",
    "\n",
    "ptb_acc_arr_inv_ptb=res.get(\"ptb\").get(\"accuracy\")\n",
    "inv_acc_arr_inv_ptb=res.get(\"inv\").get(\"accuracy\")\n",
    "clean_acc_arr_inv_ptb=res.get(\"clean\").get(\"accuracy\")\n",
    "\n",
    "for i in range(iterations):\n",
    "    data=[i,clean_acc_arr_inv_ptb[i],ptb_acc_arr_inv_ptb[i],inv_acc_arr_inv_ptb[i]]\n",
    "    writer_inv_ptb_trained.writerow(data)\n",
    "\n",
    "vanilla_model=load_model(\"models/vanilla_model\")\n",
    "\n",
    "\n",
    "\n",
    "#first PTB-Training\n",
    "res=ptb_training(ptb_acc_to_achieve, vanilla_model,use_iterations=True, iterations=iterations)\n",
    "\n",
    "ptb_acc_arr_ptb_inv=res.get(\"ptb\").get(\"accuracy\")\n",
    "inv_acc_arr_ptb_inv=res.get(\"inv\").get(\"accuracy\")\n",
    "clean_acc_arr_ptb_inv=res.get(\"clean\").get(\"accuracy\")\n",
    "\n",
    "\n",
    "for i in range(iterations):\n",
    "    data=[i,clean_acc_arr_ptb_inv[i],ptb_acc_arr_ptb_inv[i],inv_acc_arr_ptb_inv[i]]\n",
    "    writer_ptb_inv_trained.writerow(data)\n",
    "\n",
    "# then INV-Training\n",
    "print(\"INV-Training\")\n",
    "vanilla_model.fit(inv_advs_to_train,to_categorical(inv_labels_to_train,num_classes=10),\n",
    "    epochs=10,\n",
    "    verbose=0)\n",
    "\n",
    "result=test_model(vanilla_model)\n",
    "\n",
    "data=[iterations+1,result.get(\"clean\").get(\"accuracy\"),result.get(\"ptb\").get(\"accuracy\"),result.get(\"inv\").get(\"accuracy\")]\n",
    "writer_ptb_inv_trained.writerow(data)\n",
    "\n",
    "\n",
    "handler_ptb_inv_trained.close()\n",
    "handler_simultan_trained.close()\n",
    "handler_inv_ptb_trained.close()\n",
    "handler_inclusive_trained.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dritten Durchlauf evaluieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution=1\n",
    "\n",
    "handler_simultan_trained = open('data/results/dritter_durchlauf/simultan.csv', 'r')\n",
    "reader_simultan_trained = csv.DictReader(handler_simultan_trained)\n",
    "\n",
    "handler_not_simultan_trained = open('data/results/dritter_durchlauf/not_simultan.csv', 'r')\n",
    "reader_not_simultan_trained = csv.DictReader(handler_not_simultan_trained)\n",
    "\n",
    "handler_inclusive_trained = open('data/results/dritter_durchlauf/inclusive.csv', 'r')\n",
    "reader_inclusive_trained = csv.DictReader(handler_inclusive_trained)\n",
    "\n",
    "handler_inv_ptb_trained = open('data/results/dritter_durchlauf/inv_ptb.csv', 'r')\n",
    "reader_inv_ptb_trained = csv.DictReader(handler_inv_ptb_trained)\n",
    "\n",
    "handler_ptb_inv_trained = open('data/results/dritter_durchlauf/ptb_inv.csv', 'r')\n",
    "reader_ptb_inv_trained = csv.DictReader(handler_ptb_inv_trained)\n",
    "\n",
    "simultan_clean=[]\n",
    "simultan_ptb=[]\n",
    "simultan_inv=[]\n",
    "\n",
    "inclusive_clean=[]\n",
    "inclusive_ptb=[]\n",
    "inclusive_inv=[]\n",
    "\n",
    "not_simultan_clean=[]\n",
    "not_simultan_ptb=[]\n",
    "not_simultan_inv=[]\n",
    "\n",
    "inv_ptb_clean=[]\n",
    "inv_ptb_ptb=[]\n",
    "inv_ptb_inv=[]\n",
    "\n",
    "ptb_inv_clean=[]\n",
    "ptb_inv_ptb=[]\n",
    "ptb_inv_inv=[]\n",
    "\n",
    "line_count = 0\n",
    "for row in reader_simultan_trained: \n",
    "    if line_count % resolution==0:\n",
    "        simultan_clean.append(float(row[\"clean_acc\"]))        \n",
    "        simultan_ptb.append(float(row[\"ptb_acc\"]))\n",
    "        simultan_inv.append(float(row[\"inv_acc\"]))\n",
    "    line_count+=1\n",
    "\n",
    "line_count = 0\n",
    "for row in reader_not_simultan_trained:\n",
    "    if line_count % resolution==0:\n",
    "        not_simultan_clean.append(float(row[\"clean_acc\"]))\n",
    "        not_simultan_ptb.append(float(row[\"ptb_acc\"]))\n",
    "        not_simultan_inv.append(float(row[\"inv_acc\"]))\n",
    "    line_count+=1\n",
    "\n",
    "line_count = 0\n",
    "for row in reader_inclusive_trained:\n",
    "    if line_count % resolution==0:\n",
    "        inclusive_clean.append(float(row[\"clean_acc\"]))\n",
    "        inclusive_ptb.append(float(row[\"ptb_acc\"]))\n",
    "        inclusive_inv.append(float(row[\"inv_acc\"]))\n",
    "    line_count+=1\n",
    "\n",
    "\n",
    "line_count = 0\n",
    "for row in reader_inv_ptb_trained:\n",
    "    if line_count % resolution==0:\n",
    "        inv_ptb_clean.append(float(row[\"clean_acc\"]))\n",
    "        inv_ptb_ptb.append(float(row[\"ptb_acc\"]))\n",
    "        inv_ptb_inv.append(float(row[\"inv_acc\"]))\n",
    "    line_count+=1\n",
    "\n",
    "line_count = 0\n",
    "for row in reader_ptb_inv_trained:\n",
    "    if line_count % resolution==0:\n",
    "        ptb_inv_clean.append(float(row[\"clean_acc\"]))\n",
    "        ptb_inv_ptb.append(float(row[\"ptb_acc\"]))\n",
    "        ptb_inv_inv.append(float(row[\"inv_acc\"]))\n",
    "    line_count+=1\n",
    "\n",
    "\n",
    "y=[]\n",
    "j=0\n",
    "\n",
    "while j<len(simultan_clean):\n",
    "    y.append(j)\n",
    "    j+=1\n",
    "\n",
    "\n",
    "print(\"Simultan\")  \n",
    "plt.plot( y, simultan_clean, label = \"Clean\")\n",
    "plt.plot( y, simultan_ptb,label = \"PTB\")\n",
    "plt.plot( y, simultan_inv,label = \"INV\")\n",
    "plt.xlabel('Iterationen')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.xlabel(\"Iterationen\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "linear_model_clean=np.polyfit(y,simultan_clean,3)\n",
    "linear_model_clean_fn=np.poly1d(linear_model_clean)\n",
    "plt.plot(y,linear_model_clean_fn(y),color=\"#1f77b4\",label = \"Clean\")\n",
    "\n",
    "linear_model_ptb=np.polyfit(y,simultan_ptb,3)\n",
    "linear_model_ptb_fn=np.poly1d(linear_model_ptb)\n",
    "plt.plot(y,linear_model_ptb_fn(y),color=\"#ff7f0e\",label = \"PTB\")\n",
    "\n",
    "\n",
    "linear_model_inv=np.polyfit(y,simultan_inv,3)\n",
    "linear_model_inv_fn=np.poly1d(linear_model_inv)\n",
    "plt.plot(y,linear_model_inv_fn(y),color=\"#2ca02c\",label = \"INV\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "y=[]\n",
    "j=0\n",
    "\n",
    "while j<len(inclusive_clean):\n",
    "    y.append(j)\n",
    "    j+=1\n",
    "\n",
    "print(\"Inclusive\")  \n",
    "plt.plot( y, inclusive_clean, label = \"Clean\")\n",
    "plt.plot( y, inclusive_ptb,label = \"PTB\")\n",
    "plt.plot( y, inclusive_inv,label = \"INV\")\n",
    "plt.xlabel('Iterationen')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.xlabel(\"Iterationen\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "linear_model_clean=np.polyfit(y,inclusive_clean,3)\n",
    "linear_model_clean_fn=np.poly1d(linear_model_clean)\n",
    "plt.plot(y,linear_model_clean_fn(y),color=\"#1f77b4\",label = \"Clean\")\n",
    "\n",
    "linear_model_ptb=np.polyfit(y,inclusive_ptb,3)\n",
    "linear_model_ptb_fn=np.poly1d(linear_model_ptb)\n",
    "plt.plot(y,linear_model_ptb_fn(y),color=\"#ff7f0e\",label = \"PTB\")\n",
    "\n",
    "\n",
    "linear_model_inv=np.polyfit(y,inclusive_inv,3)\n",
    "linear_model_inv_fn=np.poly1d(linear_model_inv)\n",
    "plt.plot(y,linear_model_inv_fn(y),color=\"#2ca02c\",label = \"INV\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "y=[]\n",
    "j=0\n",
    "\n",
    "while j<len(not_simultan_clean):\n",
    "    y.append(j)\n",
    "    j+=1\n",
    "\n",
    "print(\"Not simultan\")\n",
    "plt.plot( y, not_simultan_clean, label = \"Clean\")\n",
    "plt.plot( y, not_simultan_ptb,label = \"PTB\")\n",
    "plt.plot( y, not_simultan_inv,label = \"INV\")\n",
    "plt.xlabel('Iterationen')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.xlabel(\"Iterationen\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "linear_model_clean=np.polyfit(y,not_simultan_clean,3)\n",
    "linear_model_clean_fn=np.poly1d(linear_model_clean)\n",
    "plt.plot(y,linear_model_clean_fn(y),color=\"#1f77b4\",label = \"Clean\")\n",
    "\n",
    "linear_model_ptb=np.polyfit(y,not_simultan_ptb,4)\n",
    "linear_model_ptb_fn=np.poly1d(linear_model_ptb)\n",
    "plt.plot(y,linear_model_ptb_fn(y),color=\"#ff7f0e\",label = \"PTB\")\n",
    "\n",
    "linear_model_inv=np.polyfit(y,not_simultan_inv,3)\n",
    "linear_model_inv_fn=np.poly1d(linear_model_inv)\n",
    "plt.plot(y,linear_model_inv_fn(y),color=\"#2ca02c\",label = \"INV\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "y=[]\n",
    "j=0\n",
    "\n",
    "while j<len(inv_ptb_clean):\n",
    "    y.append(j)\n",
    "    j+=1\n",
    "\n",
    "\n",
    "print(\"INV-PTB\")\n",
    "plt.plot( y, inv_ptb_clean, label = \"Clean\")\n",
    "plt.plot( y, inv_ptb_ptb,label = \"PTB\")\n",
    "plt.plot( y, inv_ptb_inv,label = \"INV\")\n",
    "plt.xlabel('Iterationen')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.xlabel(\"Iterationen\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "linear_model_clean=np.polyfit(y,inv_ptb_clean,3)\n",
    "linear_model_clean_fn=np.poly1d(linear_model_clean)\n",
    "plt.plot(y,linear_model_clean_fn(y),color=\"#1f77b4\",label = \"Clean\")\n",
    "\n",
    "linear_model_ptb=np.polyfit(y,inv_ptb_ptb,3)\n",
    "linear_model_ptb_fn=np.poly1d(linear_model_ptb)\n",
    "plt.plot(y,linear_model_ptb_fn(y),color=\"#ff7f0e\",label = \"PTB\")\n",
    "\n",
    "\n",
    "linear_model_inv=np.polyfit(y,inv_ptb_inv,3)\n",
    "linear_model_inv_fn=np.poly1d(linear_model_inv)\n",
    "plt.plot(y,linear_model_inv_fn(y),color=\"#2ca02c\",label = \"INV\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "y=[]\n",
    "j=0\n",
    "\n",
    "while j<len(ptb_inv_clean):\n",
    "    y.append(j)\n",
    "    j+=1\n",
    "\n",
    "print(\"PTB-INV\")\n",
    "plt.plot( y, ptb_inv_clean, label = \"Clean\")\n",
    "plt.plot( y, ptb_inv_ptb,label = \"PTB\")\n",
    "plt.plot( y, ptb_inv_inv,label = \"INV\")\n",
    "plt.xlabel('Iterationen')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.xlabel(\"Iterationen\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "linear_model_clean=np.polyfit(y,ptb_inv_clean,3)\n",
    "linear_model_clean_fn=np.poly1d(linear_model_clean)\n",
    "plt.plot(y,linear_model_clean_fn(y),color=\"#1f77b4\",label = \"Clean\")\n",
    "\n",
    "linear_model_ptb=np.polyfit(y,ptb_inv_ptb,3)\n",
    "linear_model_ptb_fn=np.poly1d(linear_model_ptb)\n",
    "plt.plot(y,linear_model_ptb_fn(y),color=\"#ff7f0e\",label = \"PTB\")\n",
    "\n",
    "\n",
    "linear_model_inv=np.polyfit(y,ptb_inv_inv,3)\n",
    "linear_model_inv_fn=np.poly1d(linear_model_inv)\n",
    "plt.plot(y,linear_model_inv_fn(y),color=\"#2ca02c\",label = \"INV\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "simultan_clean={\n",
    "    \"initial\": simultan_clean[0],\n",
    "    \"mean\":np.mean(simultan_clean),\n",
    "    \"min\":np.min(simultan_clean),\n",
    "    \"max\":np.max(simultan_clean)\n",
    "}\n",
    "\n",
    "simultan_ptb={\n",
    "    \"initial\": simultan_ptb[0],\n",
    "    \"mean\":np.mean(simultan_ptb),\n",
    "    \"min\":np.min(simultan_ptb),\n",
    "    \"max\":np.max(simultan_ptb)\n",
    "}\n",
    "\n",
    "simultan_inv={\n",
    "    \"initial\": simultan_inv[0],\n",
    "    \"mean\":np.mean(simultan_inv),\n",
    "    \"min\":np.min(simultan_inv),\n",
    "    \"max\":np.max(simultan_inv)\n",
    "}\n",
    "\n",
    "not_simultan_clean={\n",
    "    \"initial\": not_simultan_clean[0],\n",
    "    \"mean\":np.mean(not_simultan_clean),\n",
    "    \"min\":np.min(not_simultan_clean),\n",
    "    \"max\":np.max(not_simultan_clean)\n",
    "}\n",
    "\n",
    "not_simultan_ptb={\n",
    "    \"initial\": not_simultan_ptb[0],\n",
    "    \"mean\":np.mean(not_simultan_ptb),\n",
    "    \"min\":np.min(not_simultan_ptb),\n",
    "    \"max\":np.max(not_simultan_ptb)\n",
    "}\n",
    "\n",
    "not_simultan_inv={\n",
    "    \"initial\": not_simultan_inv[0],\n",
    "    \"mean\":np.mean(not_simultan_inv),\n",
    "    \"min\":np.min(not_simultan_inv),\n",
    "    \"max\":np.max(not_simultan_inv)\n",
    "}\n",
    "\n",
    "\n",
    "inclusive_clean={\n",
    "    \"initial\": inclusive_clean[0],\n",
    "    \"mean\":np.mean(inclusive_clean),\n",
    "    \"min\":np.min(inclusive_clean),\n",
    "    \"max\":np.max(inclusive_clean)\n",
    "}\n",
    "\n",
    "inclusive_ptb={\n",
    "    \"initial\": inclusive_ptb[0],\n",
    "    \"mean\":np.mean(inclusive_ptb),\n",
    "    \"min\":np.min(inclusive_ptb),\n",
    "    \"max\":np.max(inclusive_ptb)\n",
    "}\n",
    "\n",
    "inclusive_inv={\n",
    "    \"initial\": inclusive_inv[0],\n",
    "    \"mean\":np.mean(inclusive_inv),\n",
    "    \"min\":np.min(inclusive_inv),\n",
    "    \"max\":np.max(inclusive_inv)\n",
    "}\n",
    "\n",
    "print(\"SIMULTAN\")\n",
    "print()\n",
    "print(\"SIMULTAN CLEAN: initial: {}, min: {}, max: {}, mean: {}\".format(simultan_clean.get(\"initial\"),simultan_clean.get(\"min\"),simultan_clean.get(\"max\"),simultan_clean.get(\"mean\")))\n",
    "print(\"SIMULTAN PTB: initial: {}, min: {}, max: {}, mean: {}\".format(simultan_ptb.get(\"initial\"),simultan_ptb.get(\"min\"),simultan_ptb.get(\"max\"),simultan_ptb.get(\"mean\")))\n",
    "print(\"SIMULTAN INV: initial: {}, min: {}, max: {}, mean: {}\".format(simultan_inv.get(\"initial\"),simultan_inv.get(\"min\"),simultan_inv.get(\"max\"),simultan_inv.get(\"mean\")))\n",
    "print()\n",
    "print()\n",
    "print(\"NOT SIMULTAN\")\n",
    "print()\n",
    "print(\"NOT SIMULTAN CLEAN: initial: {}, min: {}, max: {}, mean: {}\".format(not_simultan_clean.get(\"initial\"),not_simultan_clean.get(\"min\"),not_simultan_clean.get(\"max\"),not_simultan_clean.get(\"mean\")))\n",
    "print(\"NOT SIMULTAN PTB: initial: {}, min: {}, max: {}, mean: {}\".format(not_simultan_ptb.get(\"initial\"),not_simultan_ptb.get(\"min\"),not_simultan_ptb.get(\"max\"),not_simultan_ptb.get(\"mean\")))\n",
    "print(\"NOT SIMULTAN INV: initial: {}, min: {}, max: {}, mean: {}\".format(not_simultan_inv.get(\"initial\"),not_simultan_inv.get(\"min\"),not_simultan_inv.get(\"max\"),not_simultan_inv.get(\"mean\")))\n",
    "print()\n",
    "print()\n",
    "print(\"INCLUSIVE\")\n",
    "print()\n",
    "print(\"INCLUSIVE CLEAN: initial: {}, min: {}, max: {}, mean: {}\".format(inclusive_clean.get(\"initial\"),inclusive_clean.get(\"min\"),inclusive_clean.get(\"max\"),inclusive_clean.get(\"mean\")))\n",
    "print(\"INCLUSIVE PTB: initial: {}, min: {}, max: {}, mean: {}\".format(inclusive_ptb.get(\"initial\"),inclusive_ptb.get(\"min\"),inclusive_ptb.get(\"max\"),inclusive_ptb.get(\"mean\")))\n",
    "print(\"INCLUSIVE INV: initial: {}, min: {}, max: {}, mean: {}\".format(inclusive_inv.get(\"initial\"),inclusive_inv.get(\"min\"),inclusive_inv.get(\"max\"),inclusive_inv.get(\"mean\")))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
